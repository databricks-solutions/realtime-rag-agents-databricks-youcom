{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22bcb9d7-4a00-4f10-ab27-dc2db4c17311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "You may find this series of notebooks at [databricks-solutions/realtime-rag-agents-databricks-youcom](https://github.com/databricks-solutions/realtime-rag-agents-databricks-youcom). For more information about this solution accelerator, visit the [blog post](https://you.com/articles/unlocking-real-time-intelligence-for-ai-agents-with-you.com-and-databricks)."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c9592f5-97eb-4bbf-8096-9df282bac703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Agent definition\n",
    "\n",
    "The UC Function from above is used as a tool within the Agent.\n",
    "\n",
    "The code below demonstrates an approach to create the MLflow Trace span for the UC Function call, and convert from the You.com `hit` structure to the expected Document structure. This [RETRIEVER](https://mlflow.org/docs/latest/genai/tracing/concepts/span/#retriever-spans) span enables the [RetrievalGroundedness scorer](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/concepts/judges/is_grounded), used in Evaluation steps in this notebook, to evaluate the content retrieved from [You.com](https://you.com)\n",
    "\n",
    "---\n",
    "Be aware of all the `NOTES` left in the code. There are some areas that you can customize your agent moving forward!"
   ]
  },
  {
   "cell_type": "code",
   "source": "import os\n\n# Set up widgets to accept job parameters\ndbutils.widgets.text(\"catalog\", \"main\", \"UC Catalog\")\ndbutils.widgets.text(\"schema\", \"default\", \"UC Schema\")\ndbutils.widgets.text(\"llm_endpoint_name\", \"databricks-claude-3-7-sonnet\", \"LLM Endpoint Name\")\n\n# Set environment variables for agent.py to use\n# agent.py reads from environment variables since it's a module that gets imported\nos.environ[\"AGENT_CATALOG\"] = dbutils.widgets.get(\"catalog\")\nos.environ[\"AGENT_SCHEMA\"] = dbutils.widgets.get(\"schema\")\nos.environ[\"AGENT_LLM_ENDPOINT\"] = dbutils.widgets.get(\"llm_endpoint_name\")\n\nprint(f\"Agent configuration:\")\nprint(f\"  Catalog: {os.environ['AGENT_CATALOG']}\")\nprint(f\"  Schema: {os.environ['AGENT_SCHEMA']}\")\nprint(f\"  LLM Endpoint: {os.environ['AGENT_LLM_ENDPOINT']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "928297a7-00e8-464c-a4a0-716eec6d5781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": "%%writefile agent.py\nfrom typing import Any, Generator, Optional, Sequence, Union\n\nimport mlflow\nimport json\nfrom databricks_langchain import (\n    ChatDatabricks,\n    VectorSearchRetrieverTool,\n    DatabricksFunctionClient,\n    UCFunctionToolkit,\n    set_uc_function_client,\n)\nfrom langchain_core.language_models import LanguageModelLike\nfrom langchain_core.runnables import RunnableConfig, RunnableLambda\nfrom langchain_core.tools import BaseTool\nfrom langgraph.graph import END, StateGraph\nfrom langgraph.graph.graph import CompiledGraph\nfrom langgraph.graph.state import CompiledStateGraph\nfrom langgraph.prebuilt.tool_node import ToolNode\nfrom mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\nfrom mlflow.pyfunc import ChatAgent\nfrom mlflow.types.agent import (\n    ChatAgentChunk,\n    ChatAgentMessage,\n    ChatAgentResponse,\n    ChatContext,\n)\nfrom mlflow.entities import SpanType, Document\nimport os\n\nmlflow.langchain.autolog()\n\n# Load configuration from environment variables or defaults\n# These are set by the notebook that imports this module\ncatalog = os.environ.get(\"AGENT_CATALOG\", \"main\")\nschema = os.environ.get(\"AGENT_SCHEMA\", \"default\")\nllm_endpoint_name = os.environ.get(\"AGENT_LLM_ENDPOINT\", \"databricks-claude-3-7-sonnet\")\n\nfunction_name = \"you_com_search_function\"\nfunction_path = f\"{catalog}.{schema}.{function_name}\"\nconnection = \"you_com_connection\"\n\n## method to make an MLflow Trace span around a UC function call.\ndef make_retriever(structured_tool):\n    def _make(func):\n        @mlflow.trace(span_type=\"RETRIEVER\")\n        def apply(*args, **kwargs):\n            resp = func(*args, **kwargs)\n            resp = json.loads(resp)\n            hits = json.loads(resp.get(\"value\", \"\")).get(\"hits\", None)\n            res = []\n            if hits is not None:\n                for hit in hits:\n                    content = {\"title\": hit[\"title\"], \n                               \"description\": hit[\"description\"],\n                               \"snippets\": hit[\"snippets\"]}\n                    metadata = {\n                        \"doc_uri\": hit[\"url\"]\n                    }\n                    res.append(Document(page_content=json.dumps(content), metadata=metadata))\n            span = mlflow.get_current_active_span()\n            span.set_outputs(res)\n            return res\n        return apply\n    \n    structured_tool.func = _make(structured_tool.func)\n    return structured_tool\n\nclient = DatabricksFunctionClient()\nset_uc_function_client(client)\n\n############################################\n# Define your LLM endpoint and system prompt\n############################################\n# NOTE - Choose a supported model serving LLM on Databricks\nllm = ChatDatabricks(endpoint=llm_endpoint_name)\n\n# NOTE - Amend System Prompt to tailor the agent to a specific use case\nsystem_prompt = \"\"\"You are a helpful AI assistant with access to real-time information through web search.\n\nYour capabilities:\n- Access current, up-to-date information through web search\n- Answer questions about recent events, breaking news, and real-time data\n- Provide accurate information with citations to sources\n\nWhen answering questions:\n1. Use the search tool to find relevant, current information\n2. Synthesize information from multiple search results when appropriate\n3. Cite your sources by mentioning the URLs or titles from search results\n4. If information is time-sensitive, mention the recency of the data\n5. Be clear when information may have changed since your search\n\nAlways strive to provide accurate, helpful, and well-sourced responses.\"\"\"\n\n###############################################################################\n## Define tools for your agent, enabling it to retrieve data or take actions\n## beyond text generation\n## To create and see usage examples of more tools, see\n## https://docs.databricks.com/generative-ai/agent-framework/agent-tool.html\n###############################################################################\ntools = []\n\n# You can use UDFs in Unity Catalog as agent tools\nuc_tool_names = [function_path]\nuc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\ntools.extend([make_retriever(t) for t in uc_toolkit.tools])\n\n\n#####################\n## Define agent logic\n#####################\n\n\ndef create_tool_calling_agent(\n    model: LanguageModelLike,\n    tools: Union[Sequence[BaseTool], ToolNode],\n    system_prompt: Optional[str] = None,\n) -> CompiledGraph:\n    model = model.bind_tools(tools)\n\n    # Define the function that determines which node to go to\n    def should_continue(state: ChatAgentState):\n        messages = state[\"messages\"]\n        last_message = messages[-1]\n        # If there are function calls, continue. else, end\n        if last_message.get(\"tool_calls\"):\n            return \"continue\"\n        else:\n            return \"end\"\n\n    if system_prompt:\n        preprocessor = RunnableLambda(\n            lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n            + state[\"messages\"]\n        )\n    else:\n        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n    model_runnable = preprocessor | model\n\n    def call_model(\n        state: ChatAgentState,\n        config: RunnableConfig,\n    ):\n        response = model_runnable.invoke(state, config)\n\n        return {\"messages\": [response]}\n\n    workflow = StateGraph(ChatAgentState)\n\n    workflow.add_node(\"agent\", RunnableLambda(call_model))\n    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n\n    workflow.set_entry_point(\"agent\")\n    workflow.add_conditional_edges(\n        \"agent\",\n        should_continue,\n        {\n            \"continue\": \"tools\",\n            \"end\": END,\n        },\n    )\n    workflow.add_edge(\"tools\", \"agent\")\n\n    return workflow.compile()\n\n\nclass LangGraphChatAgent(ChatAgent):\n    def __init__(self, agent: CompiledStateGraph):\n        self.agent = agent\n\n    def predict(\n        self,\n        messages: list[ChatAgentMessage],\n        context: Optional[ChatContext] = None,\n        custom_inputs: Optional[dict[str, Any]] = None,\n    ) -> ChatAgentResponse:\n        request = {\"messages\": self._convert_messages_to_dict(messages)}\n\n        messages = []\n        for event in self.agent.stream(request, stream_mode=\"updates\"):\n            for node_data in event.values():\n                messages.extend(\n                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n                )\n        return ChatAgentResponse(messages=messages)\n\n    def predict_stream(\n        self,\n        messages: list[ChatAgentMessage],\n        context: Optional[ChatContext] = None,\n        custom_inputs: Optional[dict[str, Any]] = None,\n    ) -> Generator[ChatAgentChunk, None, None]:\n        request = {\"messages\": self._convert_messages_to_dict(messages)}\n        for event in self.agent.stream(request, stream_mode=\"updates\"):\n            for node_data in event.values():\n                yield from (\n                    ChatAgentChunk(**{\"delta\": msg}) for msg in node_data[\"messages\"]\n                )\n\n\n# Create the agent object, and specify it as the agent object to use when\n# loading the agent back for inference via mlflow.models.set_model()\nagent = create_tool_calling_agent(llm, tools, system_prompt)\nAGENT = LangGraphChatAgent(agent)\nmlflow.models.set_model(AGENT)"
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_0c235d96-4bc7-4fb5-b118-17fd1dad0124",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-define-agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}