{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22bcb9d7-4a00-4f10-ab27-dc2db4c17311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "You may find this series of notebooks at [databricks-solutions/realtime-rag-agents-databricks-youcom](https://github.com/databricks-solutions/realtime-rag-agents-databricks-youcom). For more information about this solution accelerator, visit the [blog post](https://you.com/articles/unlocking-real-time-intelligence-for-ai-agents-with-you.com-and-databricks)."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0c9592f5-97eb-4bbf-8096-9df282bac703",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Agent definition\n",
    "\n",
    "The UC Function from above is used as a tool within the Agent.\n",
    "\n",
    "The code below demonstrates an approach to create the MLflow Trace span for the UC Function call, and convert from the You.com `hit` structure to the expected Document structure. This [RETRIEVER](https://mlflow.org/docs/latest/genai/tracing/concepts/span/#retriever-spans) span enables the [RetrievalGroundedness scorer](https://docs.databricks.com/aws/en/mlflow3/genai/eval-monitor/concepts/judges/is_grounded), used in Evaluation steps in this notebook, to evaluate the content retrieved from [You.com](https://you.com)\n",
    "\n",
    "---\n",
    "Be aware of all the `NOTES` left in the code. There are some areas that you can customize your agent moving forward!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "928297a7-00e8-464c-a4a0-716eec6d5781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile agent.py\n",
    "from typing import Any, Generator, Optional, Sequence, Union\n",
    "\n",
    "import mlflow\n",
    "import json\n",
    "from databricks_langchain import (\n",
    "    ChatDatabricks,\n",
    "    VectorSearchRetrieverTool,\n",
    "    DatabricksFunctionClient,\n",
    "    UCFunctionToolkit,\n",
    "    set_uc_function_client,\n",
    ")\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langchain_core.runnables import RunnableConfig, RunnableLambda\n",
    "from langchain_core.tools import BaseTool\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langgraph.graph.graph import CompiledGraph\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt.tool_node import ToolNode\n",
    "from mlflow.langchain.chat_agent_langgraph import ChatAgentState, ChatAgentToolNode\n",
    "from mlflow.pyfunc import ChatAgent\n",
    "from mlflow.types.agent import (\n",
    "    ChatAgentChunk,\n",
    "    ChatAgentMessage,\n",
    "    ChatAgentResponse,\n",
    "    ChatContext,\n",
    ")\n",
    "from mlflow.entities import SpanType, Document\n",
    "import yaml\n",
    "\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "cfg = yaml.safe_load(open(\"common.yaml\"))\n",
    "\n",
    "# NOTE - This is the UC function defined in `01-agent-configs`\n",
    "catalog = cfg.get(\"catalog\", \"main\") \n",
    "schema = cfg.get(\"schema\", \"default\")\n",
    "function_name = \"you_com_search_function\"\n",
    "function_path = f\"{catalog}.{schema}.{function_name}\"\n",
    "\n",
    "# NOTE - This is the connection that you defined in `01-agent-configs`\n",
    "connection = \"you_com_connection\"\n",
    "\n",
    "## method to make an MLflow Trace span around a UC function call.\n",
    "def make_retriever(structured_tool):\n",
    "    def _make(func):\n",
    "        @mlflow.trace(span_type=\"RETRIEVER\")\n",
    "        def apply(*args, **kwargs):\n",
    "            resp = func(*args, **kwargs)\n",
    "            resp = json.loads(resp)\n",
    "            hits = json.loads(resp.get(\"value\", \"\")).get(\"hits\", None)\n",
    "            res = []\n",
    "            if hits is not None:\n",
    "                for hit in hits:\n",
    "                    content = {\"title\": hit[\"title\"], \n",
    "                               \"description\": hit[\"description\"],\n",
    "                               \"snippets\": hit[\"snippets\"]}\n",
    "                    metadata = {\n",
    "                        \"doc_uri\": hit[\"url\"]\n",
    "                    }\n",
    "                    res.append(Document(page_content=json.dumps(content), metadata=metadata))\n",
    "            span = mlflow.get_current_active_span()\n",
    "            span.set_outputs(res)\n",
    "            return res\n",
    "        return apply\n",
    "    \n",
    "    structured_tool.func = _make(structured_tool.func)\n",
    "    return structured_tool\n",
    "\n",
    "client = DatabricksFunctionClient()\n",
    "set_uc_function_client(client)\n",
    "\n",
    "############################################\n",
    "# Define your LLM endpoint and system prompt\n",
    "############################################\n",
    "# NOTE - Choose a supported model serving LLM on Databricks\n",
    "LLM_ENDPOINT_NAME = \"databricks-claude-3-7-sonnet\"\n",
    "llm = ChatDatabricks(endpoint=LLM_ENDPOINT_NAME)\n",
    "\n",
    "# NOTE - Amend System Prompt to tailor the agent to a specific use case\n",
    "system_prompt = \"\"\"\"\"\"\n",
    "\n",
    "###############################################################################\n",
    "## Define tools for your agent, enabling it to retrieve data or take actions\n",
    "## beyond text generation\n",
    "## To create and see usage examples of more tools, see\n",
    "## https://docs.databricks.com/generative-ai/agent-framework/agent-tool.html\n",
    "###############################################################################\n",
    "tools = []\n",
    "\n",
    "# You can use UDFs in Unity Catalog as agent tools\n",
    "uc_tool_names = [function_path]\n",
    "uc_toolkit = UCFunctionToolkit(function_names=uc_tool_names)\n",
    "tools.extend([make_retriever(t) for t in uc_toolkit.tools])\n",
    "\n",
    "# # (Optional) Use Databricks vector search indexes as tools\n",
    "# # See https://docs.databricks.com/generative-ai/agent-framework/unstructured-retrieval-tools.html\n",
    "# # for details\n",
    "#\n",
    "# # TODO: Add vector search indexes as tools or delete this block\n",
    "# vector_search_tools = [\n",
    "#         VectorSearchRetrieverTool(\n",
    "#         index_name=\"\",\n",
    "#         # filters=\"...\"\n",
    "#     )\n",
    "# ]\n",
    "# tools.extend(vector_search_tools)\n",
    "\n",
    "\n",
    "#####################\n",
    "## Define agent logic\n",
    "#####################\n",
    "\n",
    "\n",
    "def create_tool_calling_agent(\n",
    "    model: LanguageModelLike,\n",
    "    tools: Union[Sequence[BaseTool], ToolNode],\n",
    "    system_prompt: Optional[str] = None,\n",
    ") -> CompiledGraph:\n",
    "    model = model.bind_tools(tools)\n",
    "\n",
    "    # Define the function that determines which node to go to\n",
    "    def should_continue(state: ChatAgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        last_message = messages[-1]\n",
    "        # If there are function calls, continue. else, end\n",
    "        if last_message.get(\"tool_calls\"):\n",
    "            return \"continue\"\n",
    "        else:\n",
    "            return \"end\"\n",
    "\n",
    "    if system_prompt:\n",
    "        preprocessor = RunnableLambda(\n",
    "            lambda state: [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "            + state[\"messages\"]\n",
    "        )\n",
    "    else:\n",
    "        preprocessor = RunnableLambda(lambda state: state[\"messages\"])\n",
    "    model_runnable = preprocessor | model\n",
    "\n",
    "    def call_model(\n",
    "        state: ChatAgentState,\n",
    "        config: RunnableConfig,\n",
    "    ):\n",
    "        response = model_runnable.invoke(state, config)\n",
    "\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "    workflow = StateGraph(ChatAgentState)\n",
    "\n",
    "    workflow.add_node(\"agent\", RunnableLambda(call_model))\n",
    "    workflow.add_node(\"tools\", ChatAgentToolNode(tools))\n",
    "\n",
    "    workflow.set_entry_point(\"agent\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\n",
    "            \"continue\": \"tools\",\n",
    "            \"end\": END,\n",
    "        },\n",
    "    )\n",
    "    workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "    return workflow.compile()\n",
    "\n",
    "\n",
    "class LangGraphChatAgent(ChatAgent):\n",
    "    def __init__(self, agent: CompiledStateGraph):\n",
    "        self.agent = agent\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> ChatAgentResponse:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "\n",
    "        messages = []\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                messages.extend(\n",
    "                    ChatAgentMessage(**msg) for msg in node_data.get(\"messages\", [])\n",
    "                )\n",
    "        return ChatAgentResponse(messages=messages)\n",
    "\n",
    "    def predict_stream(\n",
    "        self,\n",
    "        messages: list[ChatAgentMessage],\n",
    "        context: Optional[ChatContext] = None,\n",
    "        custom_inputs: Optional[dict[str, Any]] = None,\n",
    "    ) -> Generator[ChatAgentChunk, None, None]:\n",
    "        request = {\"messages\": self._convert_messages_to_dict(messages)}\n",
    "        for event in self.agent.stream(request, stream_mode=\"updates\"):\n",
    "            for node_data in event.values():\n",
    "                yield from (\n",
    "                    ChatAgentChunk(**{\"delta\": msg}) for msg in node_data[\"messages\"]\n",
    "                )\n",
    "\n",
    "\n",
    "# Create the agent object, and specify it as the agent object to use when\n",
    "# loading the agent back for inference via mlflow.models.set_model()\n",
    "agent = create_tool_calling_agent(llm, tools, system_prompt)\n",
    "AGENT = LangGraphChatAgent(agent)\n",
    "mlflow.models.set_model(AGENT)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_0c235d96-4bc7-4fb5-b118-17fd1dad0124",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-define-agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}