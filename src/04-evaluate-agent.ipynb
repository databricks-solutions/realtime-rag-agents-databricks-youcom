{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fa2e00c-3636-49ed-80bc-4f1cc44c35f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": "You may find this series of notebooks at [databricks-solutions/realtime-rag-agents-databricks-youcom](https://github.com/databricks-solutions/realtime-rag-agents-databricks-youcom). For more information about this solution accelerator, visit the [blog post](https://you.com/articles/unlocking-real-time-intelligence-for-ai-agents-with-you.com-and-databricks)."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db826428-3de6-4949-b537-b93c0d565ada",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Evaluate Agent\n",
    "\n",
    "Now that we have our agent define, let's evaluate it's performance based on manual traces and using [FreshQA's](https://github.com/freshllms/freshqa?tab=readme-ov-file) evaluation dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa61feb0-217b-436f-a744-2ceac320f368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Create an Evaluation Dataset from Traces\n",
    "\n",
    "MLflow Evaluation Datasets can be created via multiple approaches. The example below creates an empty Evaluation dataset then populuates from the MLflow Traces captured in the above predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c975c492-e563-46ea-9ef9-88f9e52213f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(evaluation_dataset_path):\n",
    "    assert run_id is not None, \"Run ID has not been specified. Please run the cell above\"\n",
    "\n",
    "    eval_dataset = mlflow.genai.datasets.create_dataset(\n",
    "        uc_table_name=evaluation_dataset_path,\n",
    "    )\n",
    "\n",
    "    traces = mlflow.search_traces(run_id=run_id)\n",
    "\n",
    "    eval_dataset.merge_records(traces)\n",
    "\n",
    "eval_dataset = mlflow.genai.datasets.get_dataset(uc_table_name=evaluation_dataset_path)\n",
    "display(eval_dataset.to_df())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be7576e6-40ef-4386-9635-ea0a5724aabd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Conduct Evaluation\n",
    "\n",
    "Evaluate using the evaluation dataset established in `1. Create an Evaluation Dataset from Traces`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f80c842-5a44-464d-8b6f-96302a98ee58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scorers = [\n",
    "        RetrievalGroundedness(),  # Checks if email content is grounded in retrieved data\n",
    "        Guidelines(\n",
    "            name=\"professional_tone\",\n",
    "            guidelines=\"The generated response must be in a professional tone.\",\n",
    "        ),\n",
    "        RelevanceToQuery(),  # Checks if email addresses the user's request\n",
    "        Safety(),  # Checks for harmful or inappropriate content\n",
    "    ]\n",
    "\n",
    "# Run evaluation with predefined scorers\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=lambda messages: AGENT.predict({\"messages\": messages}),\n",
    "    scorers=scorers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2a0efba-a660-4c0c-bca8-09b32056a62e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. FreshQA Evaluation\n",
    "\n",
    "Load a recent [FreshQA dataset](https://github.com/freshllms/freshqa?tab=readme-ov-file). Create a dataset of the questions and expected answers. Perform an Evaluation using the AGENT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27536353-df03-44e1-9ed5-79f64acdfb1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_or_create_freshqa_eval_dataset(spark,\n",
    "                                       table_path,\n",
    "                                       doc_id=\"1XYTgnilxwoxihTBYg9X3II4a4orHQYLAkyVnf70LA1c\", \n",
    "                                       sheet_id=\"334049794\"):\n",
    "\n",
    "  if not spark.catalog.tableExists(table_path):\n",
    "  \n",
    "    # https://github.com/freshllms/freshqa?tab=readme-ov-file\n",
    "    # FreshQA July 28, 2025 - https://docs.google.com/spreadsheets/d/1XYTgnilxwoxihTBYg9X3II4a4orHQYLAkyVnf70LA1c/edit?gid=334049794#gid=334049794\n",
    "\n",
    "    freshqa_dataset_path = f\"https://docs.google.com/spreadsheets/d/{doc_id}/export?format=csv&gid={sheet_id}\"\n",
    "    response = requests.get(freshqa_dataset_path)\n",
    "    assert response.status_code == 200, f\"Encountered error when fetching FreshQA dataset. Please check that '{freshqa_dataset_path}' exists.\"\n",
    "\n",
    "    with io.BytesIO(response.content) as csv_file:\n",
    "      pdf = pd.read_csv(csv_file, header=2)\n",
    "\n",
    "    # select the TEST split dataset\n",
    "    test_pdf = pdf[pdf[\"split\"] == \"TEST\"].copy().reset_index(drop=True)\n",
    "\n",
    "    # convert the FreshQA dataset into the format expected by mlflow.genai.evaluate\n",
    "    eval_dataset_pdf = pd.DataFrame()\n",
    "    eval_dataset_pdf[\"inputs\"] = test_pdf[\"question\"].apply(lambda x: create_message_payload(x))\n",
    "    eval_dataset_pdf[\"expectations\"] = test_pdf[\"answer_0\"].apply(lambda x: {\"expected_response\": x})\n",
    "\n",
    "    eval_dataset = mlflow.genai.datasets.create_dataset(uc_table_name=table_path)\n",
    "\n",
    "    eval_dataset.merge_records(eval_dataset_pdf)\n",
    "\n",
    "  return mlflow.genai.datasets.get_dataset(table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "618010d3-96a6-4def-8f94-6d4bc31043dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "freshqa_dataset_table_name = \"freshqa_eval_set\"\n",
    "freshqa_eval_dataset_path = f\"{catalog}.{schema}.{freshqa_dataset_table_name}\"\n",
    "\n",
    "# freshQA Google Sheet as of July 28, 2025\n",
    "# This can be updated by accessing the Google Sheet from this link\n",
    "# https://github.com/freshllms/freshqa?tab=readme-ov-file\n",
    "freshqa_doc_id = \"10c1ZhL091BQmLTQq8ryC_JQex_hKNa0r_lk2-JEDWHM\"\n",
    "freshqa_sheet_id = \"334049794\"\n",
    "\n",
    "freshqa_eval_dataset = get_or_create_freshqa_eval_dataset(spark, freshqa_eval_dataset_path, doc_id=freshqa_doc_id, sheet_id=freshqa_sheet_id)\n",
    "\n",
    "display(eval_dataset.to_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a32ca53-6bfa-478f-8b03-ef62de04c12e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "scorers = [\n",
    "        RetrievalGroundedness(),  # Checks if email content is grounded in retrieved data\n",
    "        Guidelines(\n",
    "            name=\"professional_tone\",\n",
    "            guidelines=\"The generated response must be in a professional tone.\",\n",
    "        ),\n",
    "        RelevanceToQuery(),  # Checks if email addresses the user's request\n",
    "        Safety(),  # Checks for harmful or inappropriate content\n",
    "        Correctness()\n",
    "    ]\n",
    "\n",
    "# Run evaluation with predefined scorers\n",
    "eval_results = mlflow.genai.evaluate(\n",
    "    data=freshqa_eval_dataset,\n",
    "    predict_fn=lambda messages: AGENT.predict({\"messages\": messages}),\n",
    "    scorers=scorers,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_0c235d96-4bc7-4fb5-b118-17fd1dad0124",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-evaluate-agent",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}